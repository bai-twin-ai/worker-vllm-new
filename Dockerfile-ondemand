ARG WORKER_CUDA_VERSION=11.8.0
ARG BASE_IMAGE_VERSION=1.0.0
FROM runpod/worker-vllm:base-${BASE_IMAGE_VERSION}-cuda${WORKER_CUDA_VERSION} AS vllm-base

RUN apt-get update -y \
    && apt-get install -y python3-pip git

# Install Python dependencies
COPY builder/requirements.txt /requirements.txt
RUN --mount=type=cache,target=/root/.cache/pip \
    python3 -m pip install --upgrade pip && \
    python3 -m pip install --upgrade -r /requirements.txt

ARG MODEL_NAME=""
ENV MODEL_NAME=${MODEL_NAME}
ENV MODEL_BASE_PATH="/workspace/"
ENV PORT="8000"
ENV HOST="0.0.0.0"
#ENV EXTRA_VLLM_ARGS="--gpu-memory-utilization 0.80 --chat-template vicuna_ina8_1.jinja"
#ENV EXTRA_VLLM_ARGS="--quantization awq --gpu-memory-utilization 0.80 --chat-template vicuna_v1.1.jinja"

ENV HF_DATASETS_CACHE="/workspace/huggingface-cache/datasets"
ENV HUGGINGFACE_HUB_CACHE="/workspace/huggingface-cache/hub"
ENV TRANSFORMERS_CACHE="/workspace/huggingface-cache/hub"
ENV HF_HUB_ENABLE_HF_TRANSFER="1"
ENV HUGGING_FACE_HUB_TOKEN=""

COPY src/entrypoint-ondemand.sh .
COPY templates/*.jinja .

# Start the handler
ENTRYPOINT [ "./entrypoint-ondemand.sh" ]

# Call your file when your container starts
CMD [ "python3", "-m", "vllm.entrypoints.openai.api_server" ]
